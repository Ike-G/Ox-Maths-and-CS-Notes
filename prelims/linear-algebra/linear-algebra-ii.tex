\documentclass{tikzposter} %Options for format can be included here
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{bm}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\sgn}{sgn}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}{Corollary}

\newtheorem{definition}{Definition}

\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{case}{Case}

\definecolor{nbYellow}{HTML}{FCF434}
\definecolor{nbPurple}{HTML}{9C59D1}
\definecolor{nbBlack}{HTML}{2C2C2C}
\definecolor{tBlue}{HTML}{5BCEFA}
\definecolor{tPink}{HTML}{F5A9B8}
\definecolor{bp1}{HTML}{D60270}
\definecolor{bp2}{HTML}{9B4F96}
\definecolor{bp3}{HTML}{0038A8}
\definecolor{pcs1}{HTML}{B300B3}
\definecolor{pcs2}{HTML}{54007D}
\definecolor{pcs3}{HTML}{B30086}
\definecolor{pcs4}{HTML}{3C00B3}
\definecolor{pcs5}{HTML}{2A007D}

\definecolorstyle{NewColour} {
  \definecolor{c1}{named}{nbBlack}
  \definecolor{c2}{named}{nbPurple}
  \definecolor{c3}{named}{nbYellow}
}{
  % Background Colors
  \colorlet{backgroundcolor}{black!10}
  \colorlet{framecolor}{black}
  % Title Colors
  \colorlet{titlefgcolor}{black}
  \colorlet{titlebgcolor}{black!10}
  % Block Colors
  \colorlet{blocktitlebgcolor}{c1}
  \colorlet{blocktitlefgcolor}{white}
  \colorlet{blockbodybgcolor}{white}
  \colorlet{blockbodyfgcolor}{black}
  % Innerblock Colors
  \colorlet{innerblocktitlebgcolor}{c2!80}
  \colorlet{innerblocktitlefgcolor}{black}
  \colorlet{innerblockbodybgcolor}{c2!50}
  \colorlet{innerblockbodyfgcolor}{black}
  % Note colors
  \colorlet{notefgcolor}{black}
  \colorlet{notebgcolor}{c3!50}
  \colorlet{notefrcolor}{c3!70}
}

\defineblockstyle{NewBlock}{
  titlewidthscale=1, bodywidthscale=1, titleleft,
  titleoffsetx=0pt, titleoffsety=0pt, bodyoffsetx=0pt, bodyoffsety=0pt,
  bodyverticalshift=0pt, roundedcorners=0, linewidth=0pt, titleinnersep=1cm,
  bodyinnersep=1cm
}{
  \ifBlockHasTitle%
  \draw[draw=none, fill=blocktitlebgcolor]
  (blocktitle.south west) rectangle (blocktitle.north east);
  \fi%
  \draw[draw=none, fill=blockbodybgcolor] %
  (blockbody.north west) [rounded corners=30] -- (blockbody.south west) --
  (blockbody.south east) [rounded corners=0]-- (blockbody.north east) -- cycle;
}

% Choose Layout
\usecolorstyle{NewColour}
\usebackgroundstyle{Default}
\usetitlestyle{Filled}
\useblockstyle{NewBlock}
\useinnerblockstyle[roundedcorners=0.2]{Default}
\usenotestyle[roundedcorners=0]{Default}

\settitle{\centering \color{titlefgcolor} {\Large \@title \, -- \, \@author}}

% Title, Author, Institute
\title{Linear Algebra II}
\author{Ike Glassbrook}

\begin{document}

% Title block with title, author, logo, etc.
\maketitle[titletoblockverticalspace=0.4cm]
\begin{columns}
  \column{0.5}
  \block{Determinants}{
    The determinant $D : M_{n}(\mathbb{R}) \to \mathbb{R}$ may be defined from a few initial properties:
    \begin{itemize}
      \item Multilinearity in columns:
            \begin{align*}
              D[\cdots, \bm{b}_{i} + \bm{c}_{i}, \cdots] &= D[\cdots, \bm{b}_{i}, \cdots] + D[\cdots, \bm{c}_{i}, \cdots] \\
              D[\cdots, \lambda \bm{a}_{i}, \cdots] &= \lambda D[\cdots, \bm{a}_{i}, \cdots]
            \end{align*}
      \item Alternating:
            \begin{align*}
              D[\cdots, \bm{a}_{i}, \bm{a}_{i+1}, \cdots] = 0 && \text{when } \bm{a}_{i} = \bm{a}_{i+1}
            \end{align*}
      \item $D(I_{n}) = 1$
    \end{itemize}
    This may be used to give us the following definition of the determinant:
    \begin{align*}
      D[\bm{a}_{1}, \cdots, \bm{a}_{n}] = \sum_{\sigma \in S_{n}} \sgn(\sigma) \prod_{k=1}^{n} a_{k\sigma, k}
    \end{align*}
    (this follows immediately from multilinearity and generalising the alternating property to non-adjacent columns) \\

    There are a few ways of getting the more practical inductive definition of the determinant, including just using the above formula. Generally while the practical version is computationally easier to deal with, interpreting the determinant in terms of permutations makes several properties far more obvious than otherwise. \\

    It's probably useful to note that the determinant is the factor by which areas are squared. For evidence of this, note that any function which determined how a matrix scales area must satisfy the properties of the determinant, and that there is a unique determinantal function. \\

    One of the key properties of the determinant is that it is zero for non-invertible (singular) matrices. To show this, we may first demonstrate that for any matrix $A$ and an elementary matrix $E$, $\det(EA) = \det(E) \det(A)$. Thus we observe that $\det(S_{ij}) = -1$, $\det(M_{i}(\lambda)) = \lambda$, $\det(A_{ij}(\lambda)) = 1$, so the determinant is zero if and only if the original matrix does not have $I_{n}$ as its row reduced echelon form.
  }
\block{Eigenvectors and Eigenvalues}{
  As the simplest linear maps are simply scalings in linearly independent directions, we ideally want to be able to find for any linear map a basis which makes this obvious. Thus the question becomes for which linear maps do such transformations become simple? \\

  \begin{definition} \
    A vector $v \in V$ is called an eigenvector of $T$ if $v \neq 0$ and $Tv = \lambda v$ for some $\lambda \in \mathbb{R}$. We call $\lambda \in \mathbb{R}$ an eigenvalue of $T$ if $Tv = \lambda v$ for some non-zero $v \in V$.
  \end{definition}

  \begin{claim} \
    $\lambda$ is an eigenvalue of $T$ if $\ker (T - \lambda I) \neq \{0\}$
  \end{claim}

  \begin{corollary} \
    The following statements are equivalent:
    \begin{itemize}
      \item \ $\lambda$ is an eigenvalue of $T$.
      \item \ $\ker(T - \lambda I) \neq \{0\}$.
      \item \ $T - \lambda I$ is not invertible.
      \item \ $\det (T - \lambda I) = 0$.
    \end{itemize}
  \end{corollary}
  From this we develop the notion of a characteristic polynomial -- to find the eigenvalues of $A \in M_{n}(\mathbb{R})$ is to find the roots of $\det (A - xI_{n})$. This is a degree $n$ polynomial. Note that this is well-defined for a transformation given that for an arbitrary change of basis $P(A-xI_{n})P^{-1} = PAP^{-1} - xI_{n}$, so $\det(PAP^{-1} - xI_{n}) = \det(P)\det(A-xI_{n})\det(P^{-1}) = \det(I_{n})\det(A-xI_{n})$. \\

  \begin{claim} \
    $\tr(A) = \tr(T)$ is well defined for any transformation $T$ where $A$ represents $T$ with respect to some basis.
  \end{claim}

  The proof is clear from $\tr(AB) = \tr(BA)$. \\

  \begin{claim}[Coefficients of characteristic polynomial]
    The first coefficient is $(-1)^{n}$, the second $(-1)^{n-1} \tr(A)$, and the last $\det A$.
  \end{claim}

  Proof: $\chi_{A}(0) = \det A$, and the other coefficients are found via Leibniz' formula, noticing that only the identity permutation multiplies more than $n-2$ elements along the diagonal. \\

  From this we observe that $\displaystyle \tr A = \sum_{i=1}^{n} \lambda_{i}$, and $\displaystyle \det A = \prod_{i=1}^{n} \lambda_{i}$.
}
\column{0.5}


\block{Diagonalisation}{
  The usefulness of eigenvalues and eigenvectors comes in the ability to uniquely characterise any linear map by them. We note that for any set of distinct eigenvalues, any set of corresponding eigenvectors must be linearly independent (otherwise for a minimal non-LI subset of eigenvectors, applying $T-\lambda I$ gives a smaller subset of non-LI eigenvectors). \\

  If we have a basis of eigenvectors then we may straightforwardly change coordinates to diagonalise the linear map. Thus diagonalisability is equivalent to the existence of a basis of eigenvectors. \\

  \innerblock{Geometric and algebraic multiplicity}{
    For a linear transformation $T : V \to V$, and an eigenvalue $\lambda$ for $T$,
    \[E_{\lambda} = \ker(T - \lambda I) = \{v \in V \,|\, Tv = \lambda v\}.\]
    To characterise the existence of eigenvectors and eigenvalues for $T$, we refer to the notions of geometric and algebraic multiplicity. The geometric multiplicity is the dimension of $E_{\lambda}$, while the algebraic multiplicity is the multiplicity of $\lambda$ as a root of $\chi_{T}(x)$. \\

    We have that the algebraic multiplicity must be at least the geometric multiplicity, as if we take a basis for $E_{\lambda}$ and extend it to a basis for $V$, the corresponding matrix for $T$ has top left $g_{\lambda} \times g_{\lambda}$ matrix as $\lambda I_{g_{\lambda}}$ (all zeroes below), so $\chi_{T}(x) = (\lambda - x)^{g_{\lambda}}h(x)$.
  }
}
\block{Spectral Theorem}{
  Recall from Linear Algebra I that a basis is orthonormal with respect to some inner product if for any pair of distinct bases their inner product is $0$, and the norm of any base is $1$. By the Gram-Schmidt procedure we demonstrate for real symmetric matrices that there exists an orthonormal basis of eigenvectors. \\

  Given $\{u_{1}, \hdots, u_{n}\}$ a basis for $\mathbb{R}^{n}$, we construct $\{v_{1},\hdots,v_{n}\}$ with the property that for all $k \in \{1,\hdots,n\}$, $\langle u_{1},\hdots,u_{k}\rangle = \langle v_{1},\hdots,v_{k}\rangle$.
  \begin{align*}
    w_{1} &:= u_{1} & v_{1} &:= \frac{w_{1}}{||w_{1}||} \\
    w_{2} &:= u_{2} - (u_{2} \cdot v_{1}) v_{1} & v_{2} &:= \frac{w_{2}}{||w_{2}||} \\
          &\cdots & &\cdots \\
    w_{k} &:= u_{k} - \sum_{j=1}^{k-1} (u_{k} \cdot v_{j})v_{j} & v_{k} &:= \frac{w_{k}}{||w_{k}||} \\
          &\cdots & &\cdots \\
    w_{n} &:= u_{n} - \sum_{j=1}^{n-1} (u_{n} \cdot v_{j}) v_{j} & v_{n} &:= \frac{w_{n}}{||w_{n}||}
  \end{align*}
  We may then show that all eigenvalues of a real symmetric matrix must also be real, and by induction using the Gram-Schmidt process we can generate a coordinate change producing a diagonal matrix. \\

  To see the importance of this result, note that symmetric matrices are characterised by the property that $(Au) \cdot v = u \cdot (Av)$. Further, for any inner product $\langle -, - \rangle : V^{2} \to \mathbb{R}$, a map $T : V \to V$ is self-adjoint if $\langle Tu, v \rangle = \langle u, Tv \rangle$ for all $u, v \in V$. Thus, by the spectral theorem a self-adjoint map $T$ on a finite-dimensional real inner product space $V$ has real eigenvalues, and there exists an orthonormal basis for $V$ consisting of eigenvectors of $T$. \\

  Inner products on finite-dimensional real vector spaces are a basis free way of discussing the dot product.\\

  \innerblock{Quadrics}{
    One useful application of the spectral theorem is for the purpose of classifying quadratic forms. With an equation of the form
    \[
      Q(x_{1},\dots, x_{n}) = \sum_{i=1}^{n}\sum_{j=1}^{n}a_{ij}x_{i}x_{j}=
      \begin{pmatrix}
        x_{1} & \cdots & x_{n}
      \end{pmatrix}
      A
      \begin{pmatrix}
        x_{1} \\
        \cdots \\
        x_{n}
      \end{pmatrix},
      \ A=(a_{ij})
    \]
    we can write $A$ as symmetric without loss of generality ($\frac{1}{2}(A+A^{\top})$). Consequently we may diagonalise the equation via a change of variables. \\

    Within $\mathbb{R}^{3}$, we thus get equations of the form $\mu_{1} Y_{1}^{2} + \mu_{2} Y_{2}^{2} + \mu_{3}Y_{3}^{2} = c \in \{-1, 0, 1\}$. For $\mu_{1}, \mu_{2}, \mu_{3}$ all positive, we get an ellipsoid for $c = 1$, an empty set for $c$ negative, and $\{0\}$ for $c = 0$. For one $\mu_{i}$ negative (without loss of generality, as with two negative multiply by $-1$), we get a $1$-sheet hyperboloid with $c = 1$, a $2$-sheet hyperboloid with $c = -1$, and a cone for $c = 0$ (the crossover point). \\

    The general definition of a quadric is as the set of roots in $\mathbb{R}^{3}$ of a degree 2 equation
    \begin{align*}
      f(x_{1}, x_{2}, x_{3}) &= \sum_{i = 1}^{3} \sum_{j=1}^{3} a_{ij}x_{i}x_{j} + \sum_{i=1}^{3} b_{i}x_{i} + c = 0 \\
                            &= \bm{x}^{\top} A \bm{x} + \bm{b}^{\top} \bm{x} + c && \text{for some symmetric } A \in M_{3}(\mathbb{R}) \\
                            &= \lambda_{1}y_{1}^{2} + \lambda_{2} y_{2}^{2} + \lambda_{3} y_{3}^{2} + \bm{B}^{\top}\bm{y} + c && \bm{y} = P^{\top}\bm{x} \\
                            &= \lambda_{1}Y_{1}^{2} + \lambda_{2} Y_{2}^{2} + \lambda_{3} Y_{3}^{2} + C && Y_{i} = y_{i} + \frac{B_{i}}{2\lambda_{i}}
    \end{align*}
  }
}
\end{columns}

\end{document}
