\documentclass{tikzposter} %Options for format can be included here
\geometry{paperwidth=1800mm, paperheight=2400mm}
\makeatletter
\setlength{\TP@visibletextwidth}{\textwidth-2\TP@innermargin}
\setlength{\TP@visibletextheight}{\textheight-2\TP@innermargin}
\makeatother
\usepackage{amsmath}
% \usepackage{eucal}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{parskip}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{nicefrac}
\usepackage{xcolor}
\usepackage{mathtools}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\Var}{Var}
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}
\newcommand\leftopen[2]{\ensuremath{(#1,#2]}}
\newcommand\rightopen[2]{\ensuremath{[#1,#2)}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}{Corollary}

\newtheorem{definition}{Definition}

\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{case}{Case}

\definecolor{nbYellow}{HTML}{FCF434}
\definecolor{nbPurple}{HTML}{9C59D1}
\definecolor{nbBlack}{HTML}{2C2C2C}
\definecolor{tBlue}{HTML}{5BCEFA}
\definecolor{tPink}{HTML}{F5A9B8}
\definecolor{bp1}{HTML}{D60270}
\definecolor{bp2}{HTML}{9B4F96}
\definecolor{bp3}{HTML}{0038A8}
\definecolor{pcs1}{HTML}{B300B3}
\definecolor{pcs2}{HTML}{54007D}
\definecolor{pcs3}{HTML}{B30086}
\definecolor{pcs4}{HTML}{3C00B3}
\definecolor{pcs5}{HTML}{2A007D}

\definecolorstyle{NewColour} {
  \definecolor{c1}{named}{nbBlack}
  \definecolor{c2}{named}{nbPurple}
  \definecolor{c3}{named}{nbYellow}
}{
  % Background Colors
  \colorlet{backgroundcolor}{black!10}
  \colorlet{framecolor}{black}
  % Title Colors
  \colorlet{titlefgcolor}{black}
  \colorlet{titlebgcolor}{black!10}
  % Block Colors
  \colorlet{blocktitlebgcolor}{c1}
  \colorlet{blocktitlefgcolor}{white}
  \colorlet{blockbodybgcolor}{white}
  \colorlet{blockbodyfgcolor}{black}
  % Innerblock Colors
  \colorlet{innerblocktitlebgcolor}{c2!80}
  \colorlet{innerblocktitlefgcolor}{black}
  \colorlet{innerblockbodybgcolor}{c2!50}
  \colorlet{innerblockbodyfgcolor}{black}
  % Note colors
  \colorlet{notefgcolor}{black}
  \colorlet{notebgcolor}{c3!50}
  \colorlet{notefrcolor}{c3!70}
}

\defineblockstyle{NewBlock}{
  titlewidthscale=1, bodywidthscale=1, titleleft,
  titleoffsetx=0pt, titleoffsety=0pt, bodyoffsetx=0pt, bodyoffsety=0pt,
  bodyverticalshift=0pt, roundedcorners=0, linewidth=0pt, titleinnersep=1cm,
  bodyinnersep=1cm
}{
  \ifBlockHasTitle%
  \draw[draw=none, fill=blocktitlebgcolor]
  (blocktitle.south west) rectangle (blocktitle.north east);
  \fi%
  \draw[draw=none, fill=blockbodybgcolor] %
  (blockbody.north west) [rounded corners=30] -- (blockbody.south west) --
  (blockbody.south east) [rounded corners=0]-- (blockbody.north east) -- cycle;
}

% Choose Layout
\usecolorstyle{NewColour}
\usebackgroundstyle{Default}
\usetitlestyle{Filled}
\useblockstyle{NewBlock}
\useinnerblockstyle[roundedcorners=0.2]{Default}
\usenotestyle[roundedcorners=0]{Default}

\settitle{\centering \color{titlefgcolor} {\Large \@title \, -- \, \@author}}

% Title, Author, Institute
\title{Probability, Measure and Martingales}
\author{Ike Glassbrook}

\begin{document}

% Title block with title, author, logo, etc.
\maketitle[titletoblockverticalspace=0.4cm]
\begin{columns}
  \column{0.25}
  \block{Measurable sets and functions}{
    \begin{definition}[$\sigma$-algebras]
      \ Let $\Omega$ be a set and $\mathcal{A} \subseteq \mathcal{P}(\Omega)$ be a collection of subsets of $\Omega$:
      \begin{enumerate}[label=\roman*.]
              \item $\mathcal{A}$ is an \emph{algebra} if $\varnothing \in \mathcal{A}$ and for $A, B \in \mathcal{A}$, $\mathcal{A}^{c} = \Omega \setminus A \in \mathcal{A}$ and $A \cup B \in \mathcal{A}$.
              \item $\mathcal{A}$ is a \emph{$\sigma$-algebra} if $\varnothing \in \mathcal{A}$, for $A \in \mathcal{A}$, $A^{c} \in \mathcal{A}$, and for $(A_{n})$ a sequence of sets in $\mathcal{A}$, $\bigcup_{n=1}^{\infty} A_{n} \in \mathcal{A}$.
      \end{enumerate}

    \end{definition}
    \hphantom{}

    A collection of sets is an algebra subject to being closed under finite applications of the basic operators. The $\sigma$-algebra concept extends this slightly to infinite ones. Consider where this distinction is relevant? \\

    Note that if we have $\{\mathcal{F}_{i} : i \in I\}$ are $\sigma$-algebras, then
    \begin{align*}
      \mathcal{F} = \bigcap_{i \in I} \mathcal{F}_{i}
    \end{align*}
    is a $\sigma$-algebra. This allows us to consider the notion of a smallest $\sigma$-algebra containing a set (the $\sigma$-algebra `generated' by a set). We write the $\sigma$-algebra generated by a collection of collections of sets $\mathfrak{A}$ as $\sigma(\mathfrak{A})$. \\

    \begin{definition}[Borel $\sigma$-algebra]
    \ Let $(E, \mathcal{T})$ be a topological space. The $\sigma$-algebra generated by the open sets in $E$ is called the \emph{Borel $\sigma$-algebra on $E$} and is denoted $\mathcal{B}(E) = \sigma(\mathcal{T})$.
    \end{definition}
    \hphantom{}

    \begin{definition}
    \ Suppose $(\Omega_{i}, \mathcal{F}_{i})_{i \in I}$ are measurable spaces. With $\Omega = \prod_{i \in I} \Omega_{i}$, $\mathcal{F}$ the $\sigma$-algebra generated by $A = \prod_{i \in I} A_{i}$ where $A_{i} \in \mathcal{F}_{i}$ for all $i \in I$ and for all but finitely many $i \in I$, $A_{i} = \Omega_{i}$: $(\Omega, \mathcal{F})$ is the product space.
    \end{definition}
    \hphantom{}

    This space is measurable, and $\mathcal{F}$ is a $\sigma$-algebra. \\

    \begin{definition}[$\pi$ and $\lambda$-systems]
    \ A collection of sets $\mathcal{A}$ is called a $\pi$-system if it is closed under intersections. \\

      A collection of sets $\mathcal{M}$ is called a $\lambda$-system if $\Omega \in \mathcal{M}$, if $A, B \in \mathcal{M}$, $A \subseteq B$, then $B \setminus A \in \mathcal{M}$, and if $(A_{n}) \subseteq \mathcal{M}$ with $A_{n} \subseteq A_{n+1}$ increasing then $\bigcup_{n \ge 1} A_{n} \in \mathcal{M}$.
    \end{definition}
    \hphantom{}

    A collection of sets is a $\sigma$-algebra if and only if it is both a $\pi$-system and a $\lambda$-system. \\

    \begin{lemma}[$\pi$-$\lambda$ systems lemma]
    \ Let $\mathcal{A}$ be a $\pi$-system and $\mathcal{M}$ a $\lambda$-system. Then if $\mathcal{A} \subseteq \mathcal{M}$ then $\sigma(\mathcal{A}) \subseteq \mathcal{M}$.
    \end{lemma}
    \hphantom{}

    We can use this with a convenient $\pi$-system to show that our $\lambda$-system contains more than is immediately obvious. \\

    Let $\lambda(\mathcal{A})$ be the smallest $\lambda$-system containing $\mathcal{A}$. This is a subset of $\mathcal{M}$ and $\sigma(\mathcal{A})$, so we just need to show that $\lambda(\mathcal{A})$ is a $\sigma$-algebra (for which we just have to show that it is a $\pi$-system). \\

    \begin{definition}[Random variables]
    \ With measurable spaces $(\Omega, \mathcal{F})$, $(E, \mathcal{E})$, a function $f : \Omega \to E$ is said to be an $E$-valued random variable (or a measurable function) if for all $A \in \mathcal{E}$, $f^{-1}(A) \in \mathcal{F}$.
    \end{definition}
    \hphantom{}

    We get immediately that random variables can be composed as one would expect. We can also use random variables to define new $\sigma$-algebras. Note that $(\Omega, \{f^{-1}(A) : A \in \mathcal{E}\})$ is a $\sigma$-algebra. \\

    \begin{definition}
    \ With $\{f_{i} : i \in I\}$ a family of functions $\Omega \to E$, $\sigma(f_{i} : i \in I)$ is the smallest $\sigma$-algebra on $\Omega$ for which all $f_{i}$ are measurable.
    \end{definition}
    \hphantom{}

    This is initially a slightly intimidating definition, but the intuition is just that we need our $\sigma(f_{i} : i \in I) = \sigma(f_{i}^{-1}(A) : A \in \mathcal{E}, i \in I)$. \\

    \begin{theorem}[Monotone Class Theorem]
      \ Let $\mathcal{H}$ be a class of bounded functions from $\Omega \to \mathbb{R}$ such that
      \begin{itemize}
              \item \ $\mathcal{H}$ is a vector space over $\mathbb{R}$,
              \item \ the constant function $1 \in \mathcal{H}$,
              \item \ if $(f_{n}) \subseteq \mathcal{H}$, $f_{n} \to f$ monotonically increasing, then $f \in \mathcal{H}$,
      \end{itemize}
      then if $\mathcal{C} \subseteq \mathcal{H}$, and $\mathcal{C}$ is closed under pointwise multiplication, then all bounded $\sigma(\mathcal{C})$-measurable functions are in $\mathcal{H}$.
    \end{theorem}
    \hphantom{}

    To get an intuition for this, note that any $f \in \mathcal{C}$ is necessarily bounded and $\sigma(\mathcal{C})$-measurable, but the converse is not immediate. Thus we essentially get a statement of the $\lambda$-$\pi$ systems lemma but for functions on analogous systems. \\

    We can firstly see that $\mathcal{H}$ is closed in $\mathcal{F}_{b}(\Omega)$. Then, we can prove the statement for the special case of $\mathcal{C} = \{\chi_{A} : A \in \mathcal{A}\}$ for a $\pi$-system $\mathcal{A}$, then adding $1$ to $\mathcal{C}$ without loss of generality we can make the proof more general (concretely, because $\sigma(\mathcal{C}) \subseteq \sigma(\mathcal{C} \cup \{1\})$). \\

    It may allow this theorem to make more sense to note that $\lambda$-systems are sometimes referred to as `monotone classes'. Thus the $\pi$-$\lambda$ systems lemma can be seen as saying that for $\mathcal{A}$ a $\pi$-system, the (smallest) monotone class generated by $\mathcal{A}$ is $\sigma(\mathcal{A})$. \\

    We can use the monotone class theorem to demonstrate that for $f : \Omega_{1} \times \Omega_{2} \to \mathbb{R}$ is measurable, then fixing $\omega_{1} \in \Omega_{1}$, $\omega_{2} \mapsto f(\omega_{1}, \omega_{2})$ is measurable.
  }
  \block{Conditional Probability}{
    Up until presently, we've considered the notion of event $A$ conditioned on event $B$ as having a fixed probability. This doesn't entirely capture what a conditional is however -- we're conditioning on the amount of information we have, and therefore we want the conditional probability to change as a function of our information. In particular, we want our conditional probability to be a function of $\omega \in \Omega$, and in order to reflect conditioning as a reflection of information, we want to condition over events in a $\sigma$-algebra, rather than individual events. \\

    Doing more algebra, we see that expectation is a more fitting operator, leading us to the following definition:
    \begin{definition}[Conditional expectation]
    \ Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, $X \in \mathcal{L}^{1}(\Omega, \mathcal{F}, \mathbb{P})$, $\mathcal{G} \subseteq \mathcal{F}$ a $\sigma$-algebra. A random variable $Y \in \mathcal{L}^{1}(\Omega, \mathcal{G}, \mathbb{P})$ is (a version of) the conditional expectation of $X$ given $\mathcal{G}$ if for $G \in \mathcal{G}$,
    \begin{align*}
      \mathbb{E}\big[Y \chi_{G}\big] = \mathbb{E}\big[X \chi_{G}\big].
    \end{align*}
    \end{definition}
    The key aspect of this statement can be rewritten as
    \begin{align*}
      \int_{G} \mathbb{E}\big[X \,|\, \mathcal{G}\big] \, \mathrm{d}\mathbb{P} = \int_{G} X \, \mathrm{d}\mathbb{P},
    \end{align*}
    which allows us to carry over all of our normal integration properties to conditional expectations. \\

    \begin{theorem}
    \ The conditional expectation of $X$ given $\mathcal{G}$ exists, denoted $\mathbb{E}\big[X \,|\, \mathcal{G}\big]$, and if $Z$ is also the conditional expectation of $X$ given $\mathcal{G}$, then
    \begin{align*}
      Z = \mathbb{E}\big[X \,|\, \mathcal{G}\big] \,\,\, \text{a.s.}
    \end{align*}
    \end{theorem}

    \textbf{Come back to the proof of this.} \\

    Note importantly how conditional expectations behave with respect to measurability. If we have $X$ a $\mathcal{G}$-measurable random variable, then
    \begin{align*}
      \mathbb{E}\big[X \,|\, \mathcal{G}\big] \overset{\mathrm{a.s.}}{=} X.
    \end{align*}
    Meanwhile if $\sigma(X)$ and $\mathcal{G}$ are independent, then
    \begin{align*}
      \mathbb{E}\big[X \,|\, \mathcal{G}\big] \overset{\mathrm{a.s.}}{=} \mathbb{E}\big[X\big].
    \end{align*}
    \hphantom{}

    \begin{lemma}[Tower property]
      \ Take $X \in \mathcal{L}^{1}(\Omega, \mathcal{F}, \mathbb{P})$, $\mathcal{F}_{1}$, $\mathcal{F}_{2}$ both $\sigma$-algebras, satisfying $\mathcal{F}_{1} \subseteq \mathcal{F}_{2} \subseteq \mathcal{F}$. Then
      \begin{align*}
        \mathbb{E}\Big[\mathbb{E}\big[X \,|\, \mathcal{F}_{2}\big] \,|\, \mathcal{F}_{1}\Big] \overset{\mathrm{a.s.}}{=} \mathbb{E}\big[X \,|\, \mathcal{F}_{1}\big].
      \end{align*}
    \end{lemma}

    This should be relatively intuitive -- $\mathbb{E}\big[X \,|\, \mathcal{F}_{2}\big]$ contains more information than can be represented in $\mathcal{F}_{1}$, but is fundamentally still expressing a reduced form of $X$, which can be reduced more to give $\mathbb{E}\big[X \,|\, \mathcal{F}_{1}\big]$. \\

    One can also consider this as a commutativity statement: as $\mathbb{E}\big[X \,|\, \mathcal{F}_{1}\big]$ is $\mathcal{F}_{2}$-measurable, thus $\mathbb{E}\big[X \,|\, \mathcal{F}_{1}\big] = \mathbb{E}\big[\mathbb{E}[X \,|\, \mathcal{F}_{1}] \,|\, \mathcal{F}_{2}\big]$, so the tower property is stating that with $\mathcal{F}_{1} \subseteq \mathcal{F}_{2}$:
    \begin{align*}
      \mathbb{E}\Big[\mathbb{E}\big[X \,|\, \mathcal{F}_{1}\big] \,|\, \mathcal{F}_{2}\Big] = \mathbb{E}\Big[\mathbb{E}\big[X \,|\, \mathcal{F}_{2}\big] \,|\, \mathcal{F}_{1}\Big].
    \end{align*}
    \coloredbox{
    It's tempting to claim the more general statement, that for $\mathcal{F}_{1}$, $\mathcal{F}_{2}$ both $\sigma$-algebras in $\mathcal{F}$:
    \begin{align*}
      \color{red}
      \mathbb{E}\Big[\mathbb{E}\big[X \,|\, \mathcal{F}_{1}\big] \,|\, \mathcal{F}_{2}\Big] = \mathbb{E}\big[X \,|\, \mathcal{F}_{1} \cap \mathcal{F}_{2}\big].
    \end{align*}
    This statement is true if $\mathcal{F}_{1}$ and $\mathcal{F}_{2}$ are independent, because then both sides are equal to $\mathbb{E}[X]$, but it seems that there could be a `middle-ground' between independence and containment for which commutativity stops holding.
    }

    \begin{lemma}
    \ Take $X$, $Y$ random variables on $(\Omega, \mathcal{F}, \mathbb{P})$ with $X$, $Y$, and $XY$ integrable. Then
    \begin{align*}
      \mathbb{E}\big[XY \,|\, \sigma(Y)\big] \overset{\mathrm{a.s.}}{=} Y\mathbb{E}\big[X \,|\, \sigma(Y)\big].
    \end{align*}
    \end{lemma}
    Ensure for yourself that it's clear why this implies the same holding for $\mathcal{G} \supseteq \sigma(Y)$ instead of $\sigma(Y)$.




  }


  \column{0.25}
  \block{Measures on $\mathbb{R}$}{
    \begin{definition}
      \ A measure space is a triple $(\Omega, \mathcal{F}, \mu)$ such that $\Omega$ is a set, $\mathcal{F}$ is a $\sigma$-algebra on $\Omega$, and $\mu : \mathcal{F} \to [0,\infty]$ is countably additive ($\mu$ is then a \emph{measure} on $(\Omega, \mathcal{F})$).
    \end{definition}
    \hphantom{}

    \begin{definition}
    \ Let $\mu$ be a probability measure on $\mathcal{B}(\mathbb{R})$. The distribution function of $\mu$ is $F_{\mu}(x) = \mu\leftopen{-\infty}{x}$, where we require that $F_{\mu}$ is non-decreasing, tends to $0$ as $x \to -\infty$, to $1$ as $x \to \infty$, and is right continuous.
    \end{definition}
    \hphantom{}

    \begin{definition}
    \ We say that $\nu$ is absolutely continuous with respect to $\mu$, $\nu \ll \mu$, if for any $A \in \mathcal{F}$, $\mu(A) = 0$ implies that $\nu(A) = 0$. Further, we say that $\mu$ and $\nu$ are equivalent, $\mu \sim \nu$, if $\mu \ll \nu$ and $\nu \ll \mu$.
    \end{definition}
    \hphantom{}

    \innerblock{Extensions}{
      For the most part, it's difficult to characterise a measure explicitly, due to $\sigma$-algebras being incredibly large in all but countable $\Omega$. We therefore wish to characterise them in terms of their value on algebras. \\

      \begin{theorem}[Uniqueness of extension]
      \ Let $\mu_{1}$ and $\mu_{2}$ be measures on a space $(\Omega, \mathcal{F})$, and $\mathcal{A} \subseteq \mathcal{F}$ is a $\pi$-system with $\sigma(\mathcal{A}) = \mathcal{F}$. Then if $\mu_{1}(\Omega) = \mu_{2}(\Omega) < \infty$ and $\restr{\mu_{1}}{\mathcal{A}} = \restr{\mu_{2}}{\mathcal{A}}$, then $\mu_{1} = \mu_{2}$.
      \end{theorem}
      \hphantom{}

      This follows immediately via the $\lambda$-$\pi$ systems lemma. \\

     \begin{theorem}[Carath\'{e}odory Extension theorem]
    \ Let $\Omega$ be a set and $\mathcal{A}$ an algebra on $\Omega$, then with $\mu_{0} : \mathcal{A} \to [0,\infty]$ a countably additive set function, there exists a measure $\mu : \sigma(\mathcal{A}) \to [0,\infty]$ such that $\restr{\mu}{\mathcal{A}} = \mu_{0}$.
    \end{theorem}
    \hphantom{}

    One can derive this from defining the outer measure $\mu^{*}$ in terms of $\mu_{0}$, and claiming that a set is measurable iff for all $E \subseteq \Omega$, $\mu^{*}(E) = \mu^{*}(E \cap B) + \mu^{*}(E \setminus B)$. We can then prove that this gives the smallest $\sigma$-algebra containing $\mathcal{A}$.
    }
    \hphantom{}

    \begin{definition}[Distribution function]
    \ If a function $F : \mathbb{R} \to [0,1]$ satisfies:
    \begin{enumerate}[label=\roman*.]
          \item \ $F$ is non-decreasing;
          \item \ $F(x) \to 0$ as $x \to -\infty$, $F(x) \to 1$ as $x \to \infty$; and
            \item \ $F$ is continuous from the right,
    \end{enumerate}
    then $F$ is a distribution function.
    \end{definition}
    \hphantom{}

    \begin{theorem}
    \ Let $F$ be a distribution function. Then there exists a unique Borel probability measure $\mu$ on $\mathbb{R}$ such that $\mu\leftopen{-\infty}{x} = F(x)$. Further, every Borel probability measure on $\mathbb{R}$ defines a distribution function.
    \end{theorem}
    \hphantom{}

    A corollary of this is that there is a unique Borel measure such that for all $a < b \in \mathbb{R}$, $\mu\leftopen{a}{b} = b-a$. \\

    This result demonstrates that there is a bijection between measures on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ and distribution functions. In particular, we call these measures the Lebesgue-Stieltjes measures. \\

    The proof of this theorem follows using both of the extension theorems. In particular, we use the algebra of left open right closed intervals. \\

    \begin{definition}[Pushforward measure]
      \ Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, $X : \Omega \to E$, then for $A \in \mathcal{E}$,
      \begin{align*}
        \mathbb{Q}(E) := \mathbb{P}(X^{-1}(E))
      \end{align*}
    \end{definition}
    \hphantom{}

    Thus we have a (very non-injective) map from random variables in $E$ to probability measures on $E$. This is useful, on the basis that many properties of random variables will just be properties of the pushforward measure rather than the random variable itself. \\

    \begin{theorem}
      \ For $\{(\Omega_{i}, \mathcal{F}_{i}, \mathbb{P}_{i}) : i \in \{1, \dots, n\}\}$ a set of probability measures, there is a unique measure $\mathbb{P}$ on $\displaystyle \left(\prod_{i=1}^{n} \Omega_{i},\bigtimes_{i=1}^{n} \mathcal{F}_{i}\right)$ such that for $E_{i} \in \mathcal{F}_{i}$ with $i \in \{1,\dots,n\}$,
      \begin{align*}
        \mathbb{P}\left(\prod_{i=1}^{n} E_{i}\right) &= \prod_{i=1}^{n} \mathbb{P}_{i}(E_{i}).
      \end{align*}

    \end{theorem}
    \hphantom{}

    It's hopefully natural here that one should aim an induction proof. \\

    The theorem here then allows us to extend the matter to infinite products, although at this point we require that what we're dealing with are probability measures (to keep each term in $[0,1]$ for convergence reasons), rather than just finite measures as could work with the previous statement of the theorem.
  }
  \block{Independence}{
    \begin{definition}[Independence]
      \ With $(\Omega, \mathcal{F}, \mathbb{P})$ a probability space, $(\mathcal{G}_{i})_{i=1}^{n}$ a collection of $\sigma$-algebras, these $\sigma$-algebras are independent if for $E_{i} \in \mathcal{G}_{i}$ for $i \in \{1,\dots,n\}$
      \begin{align*}
        \mathbb{P}\left(\bigcap_{i=1}^{n} E_{i}\right) &= \prod_{i=1}^{n} \mathbb{P}(E_{i}).
      \end{align*}
      Further, an arbitrary collection $(G_{i})_{i \in I}$ of $\sigma$-algebras is independent if any finite subset of the collection is independent.
    \end{definition}
    \hphantom{}

    Note that this means $\{\varnothing, \Omega\}$ is independent of anything else. \\

    Additionally, we say that a set $(X_{i})_{i \in I}$ of random variables is independent iff $(\sigma(X_{i}))_{i \in I}$ is independent. \\

    This definition requires a bit of work to deal with properly. One of the best general results we can attain quickly gives a fairly applicable result:
    \begin{theorem}
    \ With $(\Omega, \mathcal{F}, \mathbb{P})$ a probability space, $(\mathcal{A}_{i})_{i \in I}$ an arbitrary collection of $\pi$-systems, then $(\sigma(\mathcal{A}_{i}))_{i \in I}$ are independent iff for any finite $J \subseteq I$, $A_{i} \in \mathcal{A}_{i}$ for $i \in J$:
    \begin{align*}
      \mathbb{P}\left(\bigcap_{i \in J} A_{i}\right) &= \prod_{i \in J} \mathbb{P}(A_{i})
    \end{align*}
    \end{theorem}
    \hphantom{}

    We also have the result that for any independent set of $\sigma$-algebras, any subset is also independent. \\

    It takes a small bit of proving, but from the above results we get the lemma:
    \begin{lemma}
    \ With $(\Omega, \mathcal{F}, \mathbb{P})$, a family of independent random variables $X_{i} : \Omega \to E_{i}$, measurable functions $f_{i} : E_{i} \to \mathbb{R}$ for $i \in I$, then $(f(X_{i}))_{i \in I}$ are independent.
    \end{lemma}
    \hphantom{}

    \innerblock{Tail events}{
      \begin{definition}
        \ For a sequence of random variables $(X_{n})$, the tail $\sigma$-algebra is defined as
        \begin{align*}
          \mathcal{T} = \bigcap_{n = 1}^{\infty} \sigma(\{X_{k} : k > n\})
        \end{align*}

      \end{definition}
      \hphantom{}

      The intuition here is that all events in the tail $\sigma$-algebra contain sample information distinguishing the results of functions of infinitely many sequence elements. \\

      \begin{theorem}[Kolmogorov's 0-1 Law]
      \ Let $(X_{n})$ be a sequence of independent random variables. Then the tail $\sigma$-algebra of $(X_{n})$ contains only events with probability $0$ or $1$.
      \end{theorem}
      \hphantom{}

      To see this, we demonstrate that $\mathcal{T}$ is independent of a $\sigma$-algebra containing it, and therefore that all of its events are independent with themselves.
    }
    \hphantom{}

    \innerblock{Borel-Cantelli lemmas}{
      \begin{definition}
      \ With $(A_{n})$ a sequence of sets from $\mathcal{F}$:
      \begin{align*}
        \limsup_{n \to \infty} A_{n} &= \bigcap_{n=1}^{\infty} \bigcup_{m \ge n} A_{m} \\
                                     &= \{\omega \in \Omega : \omega \in A_{n} \text{ for infinitely many }n\} \\
                                     &= \{A_{n} \text{ infinitely often}\,\} \\
        \text{and} \quad \quad \liminf_{n \to \infty} A_{n} &= \bigcup_{n=1}^{\infty} \bigcap_{m \ge n} A_{m} \\
                                     &= \{\omega \in \Omega : \omega \in A_{n} \text{ eventually }\} \\
        &= \{ A_{n} \text{ eventually}\,\}
      \end{align*}
      \end{definition}
      \hphantom{}

      \begin{lemma}[Fatou and Reverse Fatou for sets]
      \ With $(A_{n})$ a sequence of sets in $\mathcal{F}$,
      \begin{align*}
        \mathbb{P}(\liminf_{n \to \infty} A_{n}) &\le \liminf_{n \to \infty} \mathbb{P}(A_{n}) \\
        \mathbb{P}(\limsup_{n\ \to \infty} A_{n}) &\ge \limsup_{n \to \infty} \mathbb{P}(A_{n}).
      \end{align*}
      \end{lemma}
      \hphantom{}

      \begin{lemma}[First Borel-Cantelli lemma]
        \ For $(A_{n})$ a sequence of events in $\mathcal{F}$, if
        \begin{align*}
          \sum_{n = 1}^{\infty} \mathbb{P}(A_{n}) < \infty,
        \end{align*}
        then $\mathbb{P}(A_{n} \text{ i.o.}) = 0$.
      \end{lemma}
      \hphantom{}

      \begin{lemma}[Second Borel-Cantelli lemma]
      \ For $(A_{n})$ a sequence of independent events in $\mathcal{F}$, if
      \begin{align*}
        \sum_{n=1}^{\infty} \mathbb{P}(A_{n}) = \infty,
      \end{align*}
      then $\mathbb{P}(A_{n} \text{ i.o.}) = 1$.
      \end{lemma}
      \hphantom{}

      By their nature, the BC lemmas are only informative in relation to almost sure events. While this may seem incredibly limited, by Kolmogorov's 0-1 Law, it turns out that many events of interest are in fact almost sure events.
    }
  }
  \column{0.25}
  \block{Integration}{
    As already covered in Part A Integration, we define integration as normal:

    \begin{definition}[Integral on simple functions]
    \ For a measure space $(\Omega, \mathcal{F}, \mu)$, $f : \Omega \to [0,\infty]$ a non-negative simple function taking values $\{a_{1},\dots,a_{n}\} \subseteq \mathbb{R}$:
    \begin{align*}
      \int f \, \mathrm{d}\mu &= \sum_{i=1}^{n} a_{i} \mu\left(f^{-1}\left(\{a_{i}\}\right)\right).
    \end{align*}
    \end{definition}
    \hphantom{}

    \begin{definition}[Integral on non-negative functions]
    \ For $f : \Omega \to [0,\infty]$ a non-negative measurable function:
    \begin{align*}
      \int f &= \sup \left\{\int g \,\mathrm{d}\mu : g \text{ simple },\, 0 \le g \le f \right\}
    \end{align*}
    \end{definition}
    \hphantom{}

    \begin{definition}[Integral]
      \ For $f : \Omega \to \mathbb{R}$ a measurable function and $\int |f| \, \mathrm{d}\mu < \infty$, we write $f^{+} = \max(f,0)$, $f^{-} = -\min(f,0)$, and
      \begin{align*}
        \int f \, \mathrm{d}\mu &= \int f^{+} \, \mathrm{d}\mu - \int f^{-} \, \mathrm{d}\mu
      \end{align*}
    \end{definition}
    \hphantom{}

    \begin{theorem}[Monotone convergence theorem]
      \ For $(f_{n})$ a sequence of non-negative functions measurable on $(\Omega, \mathcal{F}, \mu)$, such that $f_{n} \to f$ monotonically. Then
      \begin{align*}
        \int f_{n} \, \mathrm{d}\mu \to \int f \, \mathrm{d}\mu.
      \end{align*}
    \end{theorem}
    \hphantom{}

    \begin{theorem}[Fatou's lemma]
      \ For $(f_{n})$ a sequence of non-negative functions measurable on $(\Omega, \mathcal{F}, \mu)$,
      \begin{align*}
        \int \liminf_{n \to \infty} f_{n} \, \mathrm{d}\mu \le \liminf_{n \to \infty} \int f_{n} \, \mathrm{d}\mu
      \end{align*}
    \end{theorem}
    \hphantom{}

    \begin{lemma}[Reverse Fatou's lemma]
    \ For $(f_{n})$ a sequence of non-negative functions measurable on $(\Omega, \mathcal{F}, \mu)$, assume that there is an integrable function $g$ such that $f_{n} \le g$ for $n \ge 1$. Then
    \begin{align*}
      \int \limsup_{n \to \infty} f_{n} \, \mathrm{d}\mu \ge \limsup_{n \to \infty} \int f_{n} \, \mathrm{d}\mu
    \end{align*}
    \end{lemma}
    \hphantom{}

    \begin{theorem}[Dominated convergence theorem]
      \ For $(f_{n})$ a sequence of functions measurable on $(\Omega, \mathcal{F}, \mu)$ with $f_{n} \to f$ pointwise. Assume that there is an integrable function $g$ such that $|f_{n}| \le g$ for $n \ge 1$. Then
      \begin{align*}
        \int f_{n} \, \mathrm{d}\mu \to \int f \, \mathrm{d}\mu.
      \end{align*}
    \end{theorem}
    \hphantom{}

    \begin{theorem}
    \ Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, $X : \Omega \to E$, $g : E \to \mathbb{R}$ measurable on their respective spaces. Then $g$ is $(\mathbb{P} \circ X^{-1})$-integrable iff $g \circ X$ is $\mathbb{P}$-integrable. Further,
    \begin{align*}
      \int_{E} g(x) \, \mathrm{d}(\mathbb{P} \circ X^{-1})(x) = \int_{\Omega} g(X(\omega)) \, \mathrm{d}\mathbb{P}(\omega).
    \end{align*}
    \end{theorem}
    \hphantom{}

    \begin{definition}[Expectation]
    \ For $X \in \mathcal{L}^{1}(\Omega, \mathcal{F}, \mathbb{P})$,
    \begin{align*}
      \mathbb{E}[X] = \int_{\Omega} X \, \mathrm{d}\mathbb{P}.
    \end{align*}
    \end{definition}
    \hphantom{}

    We say that $X$ admits an $n$th moment if $X \in \mathcal{L}^{n}(\Omega, \mathcal{F}, \mathbb{P})$. \\

    \begin{definition}[Variance]
    \ For $X \in \mathcal{L}^{2}(\Omega, \mathcal{F}, \mathbb{P})$,
    \begin{align*}
      \Var(X) = \mathbb{E}\left[(X-\mathbb{E}[X])^{2}\right].
    \end{align*}
    \end{definition}
    \hphantom{}

    \begin{definition}[Standardised moment]
    \ If $X \in \mathcal{L}^{n}(\Omega, \mathcal{F}, \mathbb{P})$, the $n$th standardised moment of $X$ is
    \begin{align*}
      \mathbb{E}\left[\left(\frac{X - \mathbb{E}[X]}{\sqrt{\Var(X)}}\right)^{n}\right]
    \end{align*}
    \end{definition}
    \hphantom{}

    \begin{theorem}[Fubini-Tonelli]
      \ Let $(\Omega, \mathcal{F}, \mathbb{P})$ be the product of probability spaces $(\Omega_{i}, \mathcal{F}_{i}, \mathbb{P}_{i})$ for $i \in \{1,2\}$, and $f : \Omega \to \mathbb{R}$ is a bounded measurable function. Then both
      \begin{align*}
        x &\mapsto \int_{\Omega_{2}} f(x,y) \, \mathrm{d}\mathbb{P}_{2}(y) \\
        \text{and } \quad y &\mapsto \int_{\Omega_{1}} f(x,y) \, \mathrm{d}\mathbb{P}_{1}(x)
      \end{align*}
      are measurable (respectively in $\mathcal{F}_{1}$ and $\mathcal{F}_2$). \\

      If either $f \ge 0$ or $f$ is $\mathbb{P}$-integrable over $\Omega$, then
      \begin{align*}
        \int_{\Omega} f \, \mathrm{d}\mathbb{P} = \int_{\Omega_{2}} \int_{\Omega_{1}} f(x,y) \, \mathrm{d}\mathbb{P}_{1}(x) \, \mathrm{d}\mathbb{P}_{2}(y) = \int_{\Omega_{1}} \int_{\Omega_{2}} f(x,y) \, \mathrm{d}\mathbb{P}_{2}(y) \, \mathrm{d}\mathbb{P}_{1}(x)
      \end{align*}

    \end{theorem}
    \hphantom{}

    \innerblock{Radon-Nikodym theorem}{
      Integration as we've defined it gives a canonical method of defining a measure on a space: as the integral of a fixed non-negative measurable function over the set being measured. \\

      Concretely: with a measure space $(\Omega, \mathcal{F}, \mu)$, a measurable function $f : \Omega \to [0,\infty]$, $A \in \mathcal{F}$,
      \begin{align*}
        \nu(A) := \int_{A} f \, \mathrm{d}\mu
      \end{align*}
      is a measure on $\mathcal{F}$ (via MCT). \\

      We therefore want to characterise how often a measure can be characterised in this way (in particular, whether we can get this the case with respect to $\mathrm{leb}$ or the counting measure, both for which we have a wealth of tools). \\

      \begin{theorem}[Radon-Nikodym theorem]
        \ Let $\mu$, $\nu$ be two probability measures on a $\sigma$-algebra $(\Omega, \mathcal{F})$. Then $\nu \ll \mu$ if and only if there is a measurable function $f : \Omega \to [0,\infty]$ such that for $A \in \mathcal{F}$,
        \begin{align*}
          \nu(A) = \int_{A} f\, \mathrm{d}\mu.
        \end{align*}
        Further, $\nu \sim \mu$ if and only if $\mu(f^{-1}(\{0\})) = \nu(f^{-1}(\{0\})) = 0$.
      \end{theorem}
      \hphantom{}

      We call $f$ the radon-nikodym derivative of $\nu$ with respect to $\mu$, $\displaystyle f = \frac{\mathrm{d}\nu}{\mathrm{d}\mu}$. If $\nu \sim \mu$, then $\displaystyle \frac{1}{f} = \frac{\mathrm{d}\mu}{\mathrm{d}\nu}$. \\

      This means that providing $\mathrm{leb}(A) = 0$ implies that $\nu(A) = 0$, we can construct $\nu$ in this way. \\

      Using this theorem, we can define for $A, B \in \mathcal{F}$ the conditional distribution $\mathbb{P}(A \,|\, B)$, provided $\mathbb{P}(B) > 0$. If $\mathbb{P}(A) = 0$, then $\displaystyle \frac{\mathbb{P}(A \cap B)}{\mathbb{P(B)}} = 0$, so there is some $f_{B} : \Omega \to [0,\infty]$ measurable such that
      \begin{align*}
        \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} &= \int_{A} f_{B}(\omega) \, \mathrm{d}\mathbb{P}(\omega).
      \end{align*}
    }
    \hphantom{}

    We now consider modes of convergence of random variables using our results in integration. \\

    \begin{definition}[$\mathcal{L}^{p}$ spaces]
      \ For $p \ge 0$,
      \begin{align*}
        \mathcal{L}^{p}(\Omega, \mathcal{F}, \mathbb{P}) = \left\{X : \Omega \to \mathbb{R} \text{ measurable s.t. } \mathbb{E}\left[|X|^{p}\right] < \infty\right\}
      \end{align*}
      In particular, $\mathcal{L}^{0}$ is the space of all random variables, and $\mathcal{L}^{\infty}$ is the space of random variables which are bounded almost surely.
    \end{definition}
    \hphantom{}

    Note that for $0 \le p < 1$, $\mathcal{L}^{p}$ is not a normed space.

    \begin{definition}
    \ For a sequence $(X_{n})$ of random variables over $(\Omega, \mathcal{F}, \mathbb{P})$, we say that $X_{n}$ converges to $X$:
    \begin{enumerate}[label=\roman*.]
            \item almost surely ($X_{n} \overset{\mathrm{a.s.}}{\to} X$ or $X_{n} \to X$ a.s.) if
            \begin{align*}
              \mathbb{P}(X_{n} \to X \text{ as } n \to \infty) = 1.
            \end{align*}
            \item in probability ($X_{n} \overset{\mathbb{P}}{\to} X$) if for all $\varepsilon > 0$
            \begin{align*}
              \mathbb{P}(|X_{n}-X| > \varepsilon) \to 0
            \end{align*}
            as $n \to \infty$.
      \item in $\mathcal{L}^{p}$ ($X_{n} \overset{\mathcal{L}^{p}}{\to} X$) if $X_{n} \in \mathcal{L}^{p}$ for $n \ge 1$ and
          \begin{align*}
            \mathbb{E}[|X_{n}-X|^{p}] \to 0
          \end{align*}
            as $n \to \infty$.
      \item weakly in $\mathcal{L}^{1}$ if $X_{n} \in \mathcal{L}^{1}$ for $n \ge 1$ and for all $Y \in \mathcal{L}^{\infty}$
            \begin{align*}
              \mathbb{E}[X_{n}Y] \to \mathbb{E}[XY]
            \end{align*}
            as $n \to \infty$.
            \item in distribution ($X_{n} \overset{d}{\to} X$) if for $x \in \mathbb{R}$ such that $F_{X}$ is continuous,
            \begin{align*}
              F_{X_{n}}(x) \to F_{X}(x)
            \end{align*}
            as $n \to \infty$.
    \end{enumerate}
    \end{definition}
    \hphantom{}


    \begin{theorem}
      \ For a sequence $(X_{n})$ of random variables,
      \begin{enumerate}[label=\roman*.]
              \item If $X_{n} \overset{\mathrm{a.s.}}{\to} X$ then $X_{n} \overset{\mathbb{P}}{\to} X$.
              \item If $X_{n} \overset{\mathbb{P}}{\to} X$ then there is a subsequence $(X_{n_{k}})$ such that $X_{n_{k}} \overset{\mathrm{a.s.}}{\to} X$.
      \end{enumerate}
    \end{theorem}
    \hphantom{}

    \begin{lemma}[Markov's inequality]
    \ Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, $X$ a non-negative random variable. Then for $\lambda > 0$,
    \begin{align*}
      \mathbb{P}(X \ge \lambda) \le \frac{\mathbb{E}[X]}{\lambda}
    \end{align*}
    \end{lemma}
    \hphantom{}

    \begin{corollary}[General Chebyshev's Inequality]
    \ For a measurable set $A \subseteq \mathbb{R}$, $X : \Omega \to A$ a random variable, $\varphi : A \to [0,\infty]$ an increasing measurable function. For $\lambda \in A$ with $\varphi(\lambda) < \infty$ we have
    \begin{align*}
      \mathbb{P}(X \ge \lambda) \le \frac{\mathbb{E}[\varphi(X)]}{\varphi(\lambda)}.
    \end{align*}
    \end{corollary}
    \hphantom{}

    This allows us to then demonstrate that for $p > 0$, $X_{n} \overset{\mathcal{L}^{p}}{\to} X$ implies $X_{n} \overset{\mathbb{P}}{\to} X$. \\

    Further, we can also show the weak law of large numbers:
    \begin{corollary}
    \ For $(X_{n})$ be a sequence of i.i.d. random variables with mean $\mu$, variance $\sigma^{2}$, then
    \begin{align*}
      \frac{1}{n}\sum_{k=1}^{n} X_{k} \to \mu
    \end{align*}
    as $n \to \infty$.
    \end{corollary}
    \hphantom{}

    \begin{theorem}[Jensen's inequality]
      \ Let $f : I \to \mathbb{R}$ be a convex function on an interval $I \subseteq \mathbb{R}$. If $X : \Omega \to I$ is an integrable random variable then
      \begin{align*}
        \mathbb{E}[f(X)] \ge f(\mathbb{E}[X]).
      \end{align*}
    \end{theorem}
    \hphantom{}

    For considering the $\mathcal{L}^{p}$ spaces, we define $\Vert \cdot \Vert_{p} := \left(\mathbb{E}[|X|^{p}]\right)^{1/p}$. We can note immediately that for $0 \le p \le q$, $\mathcal{L}^{q} \subseteq \mathcal{L}^{p}$. \\

    Aside from this, note all the standard results regarding $\mathcal{L}^{p}$ spaces, in particular H\"{o}lder's inequality. A particular application of use is to note that for $1 < p, q < \infty$ with $1/p+1/q = 1$, and for $x > 0$,
    \begin{align*}
      x \mathbb{P}(X \ge x) \le \mathbb{E}[Y \chi_{X \ge x}],
    \end{align*}
    then $\Vert X \Vert_{p} \le q \Vert Y \Vert_{p}$.
  }
  \column{0.25}
  \block{Uniform Integrability}{
    \begin{definition}[Uniform integrability]
    \ A collection $\mathcal{C}$ of random variables is called uniformly integrable (UI) if
    \begin{align*}
      \lim_{N \to \infty} \sup_{X \in \mathcal{C}} \mathbb{E}[|X| \chi_{|X|>N}] = 0
    \end{align*}
    \end{definition}
    \hphantom{}
  }
\end{columns}

\end{document}
