\documentclass{tikzposter} %Options for format can be included here
\geometry{paperwidth=850mm, paperheight=2900mm}
\makeatletter
\setlength{\TP@visibletextwidth}{\textwidth-2\TP@innermargin}
\setlength{\TP@visibletextheight}{\textheight-2\TP@innermargin}
\makeatother
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{clrscode3e}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclareMathOperator{\Log}{Log}
\DeclareMathOperator{\rep}{rep}
\DeclareMathOperator{\res}{res}
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
    \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
      #1 % the function
      \vphantom{\big|} % pretend it's a little taller at normal size
    \right|_{#2} % this is the delimiter
  }}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}{Corollary}

\newtheorem{definition}{Definition}

\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{case}{Case}

\definecolor{nbYellow}{HTML}{FCF434}
\definecolor{nbPurple}{HTML}{9C59D1}
\definecolor{nbBlack}{HTML}{2C2C2C}
\definecolor{tBlue}{HTML}{5BCEFA}
\definecolor{tPink}{HTML}{F5A9B8}
\definecolor{bp1}{HTML}{D60270}
\definecolor{bp2}{HTML}{9B4F96}
\definecolor{bp3}{HTML}{0038A8}
\definecolor{pcs1}{HTML}{B300B3}
\definecolor{pcs2}{HTML}{54007D}
\definecolor{pcs3}{HTML}{B30086}
\definecolor{pcs4}{HTML}{3C00B3}
\definecolor{pcs5}{HTML}{2A007D}

\definecolorstyle{NewColour} {
  \definecolor{c1}{named}{nbBlack}
  \definecolor{c2}{named}{nbPurple}
  \definecolor{c3}{named}{nbYellow}
}{
  % Background Colors
  \colorlet{backgroundcolor}{black!10}
  \colorlet{framecolor}{black}
  % Title Colors
  \colorlet{titlefgcolor}{black}
  \colorlet{titlebgcolor}{black!10}
  % Block Colors
  \colorlet{blocktitlebgcolor}{c1}
  \colorlet{blocktitlefgcolor}{white}
  \colorlet{blockbodybgcolor}{white}
  \colorlet{blockbodyfgcolor}{black}
  % Innerblock Colors
  \colorlet{innerblocktitlebgcolor}{c2!80}
  \colorlet{innerblocktitlefgcolor}{black}
  \colorlet{innerblockbodybgcolor}{c2!50}
  \colorlet{innerblockbodyfgcolor}{black}
  % Note colors
  \colorlet{notefgcolor}{black}
  \colorlet{notebgcolor}{c3!50}
  \colorlet{notefrcolor}{c3!70}
}

\defineblockstyle{NewBlock}{
  titlewidthscale=1, bodywidthscale=1, titleleft,
  titleoffsetx=0pt, titleoffsety=0pt, bodyoffsetx=0pt, bodyoffsety=0pt,
  bodyverticalshift=0pt, roundedcorners=0, linewidth=0pt, titleinnersep=1cm,
  bodyinnersep=1cm
}{
  \ifBlockHasTitle%
  \draw[draw=none, fill=blocktitlebgcolor]
  (blocktitle.south west) rectangle (blocktitle.north east);
  \fi%
  \draw[draw=none, fill=blockbodybgcolor] %
  (blockbody.north west) [rounded corners=30] -- (blockbody.south west) --
  (blockbody.south east) [rounded corners=0]-- (blockbody.north east) -- cycle;
}

% Choose Layout
\usecolorstyle{NewColour}
\usebackgroundstyle{Default}
\usetitlestyle{Filled}
\useblockstyle{NewBlock}
\useinnerblockstyle[roundedcorners=0.2]{Default}
\usenotestyle[roundedcorners=0]{Default}

\settitle{\centering \color{titlefgcolor} {\Large \@title \, -- \, \@author}}

% Title, Author, Institute
\title{Algorithms and Data Structures}
\author{Ike Glassbrook}

\begin{document}

% Title block with title, author, logo, etc.
\maketitle[titletoblockverticalspace=0.4cm]
\begin{columns}
  \column{0.5}
  \block{Amortised Analysis}{
    In order to analyse data structures, it's insufficient to characterise their corresponding algorithms in terms of their worst case performance without context. While this is fine to do when analysing an algorithm over variables that can be arbitrarily valued, algorithms for data structures can be analysed with more context (i.e. some knowledge of the previous operations that were performed on the data structure). To do this, we perform amortised analysis, where instead of determining the worst case performance for each data structure operation, we get an upper bound on the average cost of an arbitrary sequence of operations. \\

    Note that although this is an average over operations, this is distinct from average-case analysis, where one finds the expected performance given a probability distribution over inputs, as the upper bound is over all possible sequences. \\

    There are three methods of performing amortised analysis:
    \begin{itemize}
            \item \ Aggregate analysis, where we get an upper bound on the total cost of a sequence of $n$ operations, then divide by $n$.
            \item \ The accounting method, where we amortise the cost of each operation by modifying it, such that for any sequence of operations the sum of modified costs is no lower than the sum of real costs. If chosen deliberately this should simplify analysis.
            \item \ The potential method, where we define a non-negative potential function $\varphi$ on the states of the data structure, measuring the `entropy' of the structure at each point, allowing us to amortise the cost of operations by how they alter the entropy (do they make other operations easier or more complex to perform?). \\
    \end{itemize}

    Arguably, the accounting method is a more general means of performing aggregate analysis, by simplifying the problem to one that can be dealt with through aggregate methods, and the potential method is itself a general version of the accounting method. \\

    More precisely, we want a $C$ such that for any $n$, sequence $c_{1},\dots,c_{n}$, we have
    \begin{align*}
      C \ge \frac{1}{n}\sum_{k=1}^{n} c_{k}.
    \end{align*}
    In certain cases we can immediately do aggregate analysis, and just observe a bound algebraically. This is especially easy when we only need to consider one or two operations with a straightforward or no algebraic relationship with one another. In other situations when we have more operations that have the possibility of interacting differently with one another it can be far more difficult to do this and still get a reasonable bound. \\

    In more difficult cases, we might want to transform the sequence to $\widehat{c}_{1}, \dots, \widehat{c}_{n}$ such that
    \begin{align*}
      \sum_{k=1}^{n} \widehat{c}_{k} \ge \sum_{k=1}^{n} c_{k}.
    \end{align*}
    In order for this to work for an arbitrary sequence of operations, we redefine the cost of each operation by subtracting from high cost operations some degree of credit, which we pass on as additional charges to more frequent low cost operations. To do this one needs to make an argument that the charges account for the credit, but once this argument is made usually the amortised cost is $O(\max \widehat{c}_{k})$. \\

    As an example, take a stack with the operations $\proc{Push}(S,x)$, $\proc{Pop}(S)$, and $\proc{Multipop}(S,m)$ (to pop up to $m$ elements at once). $\proc{Push}$ and $\proc{Pop}$ are both of cost $1$, while $\proc{Multipop}$ has cost $\min(n,m)$ where $S$ is of size $n$. For each $\proc{Push}$ we can add an additional charge of $2$, in exchange for a credit of $1$ for each $\proc{Pop}$, and a credit of $\min(n,m)$ for each $\proc{Multipop}$. This makes $\proc{Push}$ the only operation for which a cost is actually assigned, giving $O(1)$ amortised cost. \\

    In other cases, it's easier to conceptualise the amortisation not as credit for prepaid charges, but as the cost of each operation in addition to the degree by which it `complicates' the state of the data structure. Strictly, with the set of configurations of the data structure being $\mathcal{C}$, we have $\varphi : \mathcal{C} \to [0,\infty)$, and then define the modified costs for a sequence of costs $c_{1},\dots,c_{n}$, states $s_{0},\dots,s_{n}$ by
    \begin{align*}
      \widehat{c}_{k} &= c_{k} + \varphi(s_{k}) - \varphi(s_{k-1}),
    \end{align*}
    which gives
    \begin{align*}
      \sum_{k=1}^{n} \widehat{c}_{k} &= \varphi(s_{n}) - \varphi(s_{0}) + \sum_{k=1}^{n} c_{k}.
    \end{align*}
    Thus as we take $\varphi(s_{0}) = 0$ by default, this gives an upper bound on the aggregate cost, and for any appropriate measure of the entropy of the data structure, we only need to understand how the entropy is changed by a new operation. Without working through details, an example of a $\varphi$ for an $m$-bit number might be the number of digits equal to $1$.
  }

  \block{Disjoint sets}{
    A commonly required data structure is one that represents a collection of disjoint sets, used for various applications such as Kruskal's algorithm for finding the minimum spanning tree of a graph, and the unification algorithm (\textbf{confirm how this is used?}). \\

    Strictly, we want to store and update $\mathcal{F} = \{S_{1}, \dots, S_{n}\}$ where for $i \neq j$, $S_{i} \cap S_{j} = \varnothing$. To do this we identify each set by its representative $\mathrm{rep}(S)$, which ought to be consistent (if we request $\mathrm{rep}(S)$ multiple times without modifying the set, we would expect the same answer). By default we hold 3 different operations: $\proc{Make-Set}(x)$, $\proc{Find-Set}(x)$, and $\proc{Union}(x,y)$. \\

    There are a wealth of different options for implementing this data structure. One immediate approach is to store each set as a linked list of elements. To speed up $\proc{Find-Set}$, we need each object to point to the list head, making the operation constant. To perform $\proc{Union}$, we will always need to iterate through all elements of one of the sets to update their head pointer, but we can prevent the need to do both by storing a pointer to the tail of each list. Either way $\proc{Union}$ is $\Theta(n)$, and our amortised cost is $\Theta(n)$. \\

    An improvement to this can be made by always preferring that the new representative after the union is the one of the larger set, and thus we only need to iterate over the smaller set. To do this we need an additional store of the size of each set, which has no effect on performance. This allows us to get $O(\log n)$ amortised cost, by observing that each object's pointer is updated $O(\log n)$ times across $n$ union operations. \\

    An alternative to the linked list approach is to use trees to represent each set instead. This simplifies $\proc{Union}$ to two $\proc{Find-Set}$ operations from which you have one root inserted as the child of another, although this only improves performance if $\proc{Find-Set}$ is reasonably fast, which it isn't without improvements (currently worst-case $\Omega(n)$). We apply two heuristics which give our desired improvements:
    \begin{itemize}
    \item \ \emph{Union by rank}, where we store the rank of each set (or an upper bound on it, at least), and ensure that the tree with a lower rank becomes the child of the other one.
    \item \ \emph{Path compression}, where $\proc{Find-Set}$ involves updating the pointer of each element traversed to the root. \\
    \end{itemize}

    On its own, union by rank guarantees that rank will only be increased if both sets have equal rank, meaning for $n$ elements the rank is at most $\log n$, so $\proc{Find-Set}$ and $\proc{Union}$ take $O(\log n)$ time. \\

    Combining union by rank with path compression gives an extremely good bound of $O(\log^{*} n)$ amortised performance. The proof of this follows from a potential function defined by the sum of the potentials of each set element, noting that $\proc{Union}$ and $\proc{Find}$ operations will only decrease the entropy of the structure, and if they do it will be by a significant amount. This bound is so good that in practice it's constant. \\
  }


 \block{Binary Search Trees}{
   Binary search trees are used to maintain a dynamic set of orderable elements. Note they are similar but distinct from max heaps, as an inorder search returns their ordering, while for a max heap we only have a partial ordering from each parent to each child. \\

   Typically we endow a BST with the operations $\proc{Search}(x)$, $\proc{Insert}(x)$, $\proc{Delete}(x)$, $\proc{Successor}(x)$. These all behave as expected -- note that $\proc{Successor}$ returns the next largest value in the entire tree. All of these have running time $O(h)$ where $h$ is the height of the tree. \\

   Ideally we should just have that the tree is balanced, giving $O(\log n)$ amortised complexity. This of course isn't guaranteed in all implementations, so without extra work we get $O(n)$ complexity in a worst case. The key problem to deal with is one where we insert elements in order, which has the potential to create a tree with a single long branch. \\

   \innerblock{Red-Black Trees}{
     One way of ensuring that trees are balanced is using Red-Black trees. These are BSTs which identify each node as either red or black, with the following conditions:
     \begin{itemize}
             \item \ The root is black;
             \item \ Every leaf ($\const{NIL}$) is black;
             \item \ If a node $x$ is red, $\mathrm{left}[x]$ and $\mathrm{right}[x]$ are both black;
             \item \ For each node $x$, all paths from $x$ to any descendant pass through the same number of black nodes.
     \end{itemize}
     As convention we say that every valued node has 2 children. The above allows us to write without loss of generality the \emph{black-height} for $x$, $\mathrm{bh}(x)$, as the number of black nodes on a path from $x$ to a leaf (not including $x$).\\

   \begin{theorem}
   \ A red-black tree $T$ with $n$ items has height $\le 2 \log n$
   \end{theorem}
   \hphantom{}

   Firstly, any subtree rooted at node $x$ has at least $2^{\mathrm{bh}(x)}$ nodes, as collapsing each red node into its parent guarantees a tree of height $\mathrm{bh}(x)$ for which every node has $\ge 2$ children. Further, at least half of the nodes on any path must be black, so $\mathrm{height}(x) \le 2 \mathrm{bh}(x)$, and thus with $n$ nodes we have $\mathrm{height}(x) \le 2 \log n$. \\

   To implement a tree obeying these properties, we need to use rotations to restructure the tree while maintaining the BST properties. Given a subtree with root $y$, $\mathrm{left}[y] = x$, with $\alpha$, $\beta$ the descendent trees of $x$ and $\gamma$ the right descendent tree of $y$, a right rotation about $y$ moves $x$ to the root, $\mathrm{right}[x] = y$, and $\beta$ the left subtree of $y$. A left rotation behaves identically in the symmetric way. \\

   In order to insert into a RB tree, we insert the element as normal, then colour it red so as to initially preserve the black height. This gives a few cases where there is a red-red violation:
   \begin{itemize}
           \item \ We could have that the uncle element is red, in which case we just recolour the parent and uncle, then recolour their parent. Then we've moved the problem from $p[x]$ to $p[p[x]]$, and can repeat.
           \item \ We could have that the uncle element is black and the triple $x$, $p[x]$, $p[p[x]]$ occur in a zig-zag. Then we rotate $x$ with $p[x]$ to bring us to the next case.
           \item \ We could have that the triple occurs in a straight line, in which case we rotate $p[x]$ with $p[p[x]]$ and recolour. \\
   \end{itemize}

   In total this gives us $O(\log n)$ time (as case 2 and 3 occur at most once, because case 2 leads to case 3 which leads to termination), allowing us to get everything in $O(\log n)$. Deletions are more complicated, although also $O(\log n)$, but not directly considered in this course.
   }
   \hphantom{}

   In certain cases, we can do better by considering the likelihood of particular elements being accessed. Given a distribution over the frequency of elements being accessed, we write $T^{*}$ as the statically optimal BST which gives the minimum aggregate look-up cost. \\

   We have from Knuth that there is an $O(n^{2})$ DP algorithm to solve this, and from Mehlhorn that there is an $O(n \log n)$ algorithm which achieves at most a factor of $3/2$ over the optimal. Both require knowing the distribution beforehand however. \\

   \innerblock{Splay trees}{
   We introduce Splay trees as self-adjusting BSTs. The key idea is that when accessing an item $x$, move it to the root via a sequence of rotations. Thereby a tree develops for which the most accessed items stay near the root, and the least access items are the furthest away. \\

   Until $x$ is the root of $T$, if $x$ has a parent $y$ but no grandparent, then we rotate $x$ with $y$. Otherwise if the triple $x$, $p[x]$, $p[p[x]]$ is aligned, rotate $p[x]$ with $p[p[x]]$ and then $x$ with $p[x]$, and if it is not aligned rotate $x$ with $p[x]$ then $x$ with $p[p[x]]$. \\

   The main point to notice here is that in the zig-zig procedure (where $x$, $p[x]$, and $p[p[x]]$ are aligned) ensures that surrounding vertices are brought up at the same time as the vertex being searched for. If a double rotation was used instead, there would be no wider improvement made to the tree, meaning certain sequences of searches would be $\Omega(n)$ amortised rather than $O(\log n)$. \\

   We need to make some modifications to the other BST operations -- in $\proc{Insert}(T, x)$ we insert as normal, then $\proc{Splay}(T, x)$. In $\proc{Delete}(T, x)$ we $\proc{Splay}(T, x)$, remove $x$, then take the furthest right element on the left $w = \max(T_{<x})$, $\proc{Splay}(T_{<x}, w)$, and join $T_{<x}$ and $T_{>x}$. \\

   Via $\varphi(T) = \sum_{x \in T} \log |T_{x}|$, we track the potential of a state as the sum of element ranks. This eventually, after far more technical detail than I want to write, gives that an empty Splay tree has $O((m+n) \log n)$ cost after $n$ $\proc{Insert}$ and $m$ $\proc{Delete}/\proc{Search}$ operations. To replicate the proof, note that $\log |T_{x}|$ is the rank of $x$, and then determined the amortised cost in terms of $r(x)$ and $r'(x)$ for $\proc{Splay}(T,x)$. \\

   In fact, the same proof method can be used with each element's contribution to the rank weighted by its access frequency to show that the cost of Splay trees is within a constant factor of the cost of an optimal static tree for any distribution. Note that whether the same is true for an optimal dynamic tree (one where the tree is permitted to change during operations) is open.
   }
  }


\column{0.5}
\block{Flow Networks}{
  Flow networks are an abstraction to capture networks where edges capture some sort of traffic, and nodes act as switches passing traffic. Formally, it is a tuple $(G, s, t, c)$ where $G = (V,E)$ is a directed graph, we have a source $s \in V$, a sink $t \in V$, and a capacity function $c : E \to \mathbb{Z}_{\ge 0}$. \\

  For simplicity we assume that there are no anti-parallel edges (edges $(v,u)$ and $(u,v)$), that no edge enters $s$, and no edge leaves $t$. \\

  An $s$-$t$ cut is a partition of $V$ into two sets $A, B$ such that $s \in A$, $t \in B$. The capacity of an $s$-$t$ cut is the sum of the capacities of edges exiting $A$. The minimum cut problem is that of finding an $s$-$t$ cut of minimum capacity. \\

  A flow is an assignment $f : E \to \mathbb{R}_{\ge 0}$ where for each $e \in E$, $f(e) \le c(e)$, and for each $v \in V \setminus \{s, t\}$, the sum of $f(e)$ for $e$ going into $v$ is the sum of $f(e)$ for $e$ out of $v$. The value of $f$ is the sum of $f(e)$ for $e$ out of $s$. The maximum flow problem is that of maximising the flow value. \\

  With $(G, s, t, c)$ a flow network, then for a flow $f$, $s$-$t$ cut $(A, B)$, then
  \begin{align*}
    |f| &= \sum_{\text{$e$ out of $s$}} f(e) \\
        &= \sum_{v \in A \setminus \{s\}} \left(\sum_{\text{$e$ out of $v$}} f(e) - \sum_{\text{$e$ into $v$}} f(e)\right) + \sum_{\text{$e$ out of $s$}} f(e) \\
        &= \sum_{v \in A} \left(\sum_{\text{$e$ out of $v$}} f(e) - \sum_{\text{$e$ into $v$}} f(e)\right) \\
        &= \sum_{\text{$e$ out of $A$}} f(e) - \sum_{\text{$e$ into $A$}} f(e) \\
  \end{align*}

  Hence $|f| \le c(A,B)$ (the capacity of the $(A,B)$ cut). Thus we get weak duality, that if $|f| = c(A,B)$, then $f$ is maximum flow. \\

  We have the idea of a residual graph for a greedy method of maximising flow. With respect to a specific flow, we construct $G_{f} = (V, E_{f})$, where for each $e \in E$, if $f(e) < c(e)$ we have a forwards edge $e$ to $E_{f}$ with $c_{f}(e) = c(e) - f(e)$, and if $0 < f(e)$, then we had a backwards edge $e' = \mathrm{rev}(e)$ with $c_{f}(e') = f(e)$. Thus we can trace from $t$ the flow back to $s$, and the forward edges represent lost flow which could move through that edge. \\

  Take a path from $s$ to $t$ in the residual graph - this uses the edges along which flow was lost, ensuring we can make an improvement to $|f|$. Thus with $b_{P}$ the minimum $c_{f}(e)$ for $e \in P$, if $e \in P$ set $f'(e) = f(e) + b_{P}$, and if $\mathrm{rev}(e) \in P$ set $f'(e) = f(e) - b_{P}$. This constructs a new valid flow. \\

  The $\proc{Ford-Fulkerson}$ algorithm does precisely this. It sets $f = 0$ initially, constructs the residual graph $G_{f}$, then applies the above augmentation to $f$, updates $G_{f}$, then repeats until there is no $s$-$t$ path in $G_{f}$. For each iteration, we increase $|f|$ by the bottleneck capacity of $P$, which is at least $1$, so it takes at most $|f^{*}|$ iterations to complete the while loop. Further, the execution of the loop takes $O(m)$ time. Note that this only terminates necessarily because the capacities are integers (\textbf{why?}).  \\

  Upon termination, $G_{f}$ has no $s$-$t$ path, so disconnected. The corresponding cut $(A,B)$ has $c(A, B) = |f|$, so the algorithm returns the max flow. This is true because for any forward edge crossing $(A,B)$, $f(e) = c(e)$ (as otherwise we would have a forward edge across the cut), and for any backwards edge crossing $(A,B)$ we have $f(e) = 0$ (as otherwise, again, we get a forward edge across the cut). Thus $|f| = c(A,B)$.\\

  In order to choose the $s$-$t$ path $P$ to reduce the time complexity, we want to use the $P$ with maximum bottleneck capacity, so to increase $|f|$ by as much as we can at each iteration. One option would be to adapt Dijkstra's algorithm, and the other would be to do a binary search on the value of $b_{P}$. With $\Delta \ge 1$, let $G_{f}(\Delta)$ be the subgraph of $G_{f}$ with edge capacities $\ge \Delta$. We use the following algorithm:

  \begin{codebox}
  \Procname{$\proc{Capacity-Scaling}(G, s, t, c)$}
  \li Set $f(e) = 0$ for all $e \in E$
  \li $C = \max_{e \in E} c(e)$
  \li $\Delta \gets \max \{2^{n} \,|\, 2^{n} \le C\}$
  \li \While $\Delta \ge 1$ \Do
  \li     Compute $G_{f}(\Delta)$
  \li     \While there exists $s$-$t$ path $P$ in $G_{f}(\Delta)$ \Do
  \li         $f \gets \proc{Augment}(f, P)$
  \li         Update $G_{f}(\Delta)$
          \End
  \li     $\Delta \gets \Delta / 2$
      \End
  \li \Return $f$
  \end{codebox}

  It turns out that we can actually remove runtime dependence on $C$ entirely however, by the $\proc{Edmonds-Karp}$ algorithm which just selects the path with the fewest edges at each point of the iteration.  \\

  \textbf{Make sure to analyse the runtime of all the above} \\

  In summary, we get
  \begin{itemize}
          \item \ $\proc{Ford-Fulkerson}$ runs in time $O(mnC)$.
          \item \ $\proc{Capacity-Scaling}$ runs in time $O(m^{2}\log C)$.
          \item \ $\proc{Edmonds-Karp}$ runs in time $O(m^{2}n)$.
          \item \ State of the art algorithms are $O(mn)$.
  \end{itemize}

  Given an undirected bipartite graph $(X \cup Y, E)$, $(X, Y)$ denotes the bipartition of the vertex set of $G$. Note any edge is of the form $(x, y)$ for some $x \in X$, $y \in Y$. With a cost function $c : E \to [0,\infty)$, for a set $S \subseteq E$ we write $c(S)$ as the sum cost of $S$, and we want to find the minimum cost perfect matching (a matching where every vertex appears exactly once).
}

\block{Linear Programming}{
  A linear programming problem is one involving maximisation or minimisation of a cost function subject to constraints formed of weak inequalities and equality. The maximum flow problem is itself a linear programming problem. \\

  We say that a linear program is feasible if there is a solution, infeasible otherwise. The fundamental theorem of linear programming states that for each LP, either the LP is infeasible, it is feasible but unbounded, or it is bounded, feasible, and there is a solution. \\

  To solve LPs naively we initially draw out the feasible set, then calculate the cost manually at each intersection point. To do this more systematically, we can try to pick multipliers for each equality to solve a different LP. This ends up being the dual LP. \\

  Written precisely: a general LP consists of $n$ real variables $x_{1},\dots,x_{n}$, a linear objective function to be maximised or minimised, a set of $\le $ constraints upper bounded by some $b_{i}$ for $i \in \{1,\dots,m\}$, a set of $= $ constraints equal to $b_{i}$ for $i \in \{m+1,\dots,p\}$, and a set of $\ge$ constraints lower bounded by some $b_{i}$ for $i \in \{p+1,\dots,q\}$. \\

  We convert initially to maximum standard form by multiplying the objective function by $-1$, replacing equality constraints by two inequality constraints, convert each inequality constraint to a $\le $ inequality, and make all variables non-negative by writing each $x$ as $x^{+}-x^{-}$ for both positive. \\

  Having got the maximum standard form, we multiply each constraint by $y_{i} \ge 0$, sum the inequalities to obtain
  \begin{align*}
    \sum_{i=1}^{m} y_{i}\left(\sum_{j=1}^{n} a_{ij}x_{j}\right) \le \sum_{i=1}^{m} b_{i}y_{i}
  \end{align*}
  and we ensure that the objective function $\sum_{i=1}^{n} c_{i}x_{i} \le L$ by having $c_{j} \le \sum_{i=1}^{m} a_{ij}y_{i}$, and then our goal is just to minimise the upper bound in the $y_{i}$s
}
\block{Approximation Algorithms}{
  We say that algorithm $\mathcal{A}$ is a $\rho$-approximation algorithm for a minimisation problem $\mathcal{P}$ if for every instance $x$ of $\mathcal{P}$, $\mathcal{A}(x) \le \rho \cdot \mathrm{OPT}(x)$. In the opposite way, algorithm $\mathcal{A}$ is a $\rho$-approximation algorithm for a maximisation problem $\mathcal{P}$ if for every instance $x$ of $\mathcal{P}$ we have $\mathcal{A}(x) \ge \rho \cdot \mathrm{OPT}(x)$. \\

  There are 3 main approximation techniques:
  \begin{itemize}
    \item Combinatorial algorithms, which use counting methods to find a separate bound.
    \item LP rounding, wherein we express the problem using an integer linear program, then relax the integrality constraints and solve the standard LP, then round and argue that the integral solution is not far from the optimal.
    \item Randomisation, wherein we use probabilistic arguments to allow us to make more general choices and argue from probability that they give good results.
  \end{itemize}

}

\end{columns}

\end{document}
